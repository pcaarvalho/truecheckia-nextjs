import { openaiClient, OPENAI_SETTINGS, estimateTokens, estimateCost } from './client'
import { AnalysisIndicator, SuspiciousPart } from '@/app/types/analysis'
import { AppError, ERROR_CODES } from '../middleware'

// Analysis prompt templates
const ANALYSIS_PROMPTS = {
  pt: {
    system: `Você é um especialista em detecção de texto gerado por IA. Analise o texto fornecido e determine se foi escrito por um humano ou gerado por inteligência artificial.

Retorne APENAS um JSON válido no seguinte formato:
{
  "aiScore": number (0-100, onde 100 = definitivamente IA),
  "confidence": "HIGH" | "MEDIUM" | "LOW",
  "isAiGenerated": boolean,
  "indicators": [
    {
      "type": "string",
      "description": "string",
      "severity": "low" | "medium" | "high"
    }
  ],
  "explanation": "string (explicação detalhada em português)",
  "suspiciousParts": [
    {
      "text": "string (trecho suspeito)",
      "score": number (0-100),
      "reason": "string (motivo da suspeita)"
    }
  ]
}

Indicadores importantes:
- Repetição de frases ou estruturas
- Uso excessivo de conectores ("além disso", "portanto", "em conclusão")
- Linguagem muito formal ou artificial
- Falta de opinião pessoal ou experiência
- Estrutura muito organizada e previsível
- Uso de listas numeradas excessivas`,
    
    user: (text: string) => `Analise este texto e determine se foi gerado por IA:\n\n"${text}"`
  },
  
  en: {
    system: `You are an expert in AI-generated text detection. Analyze the provided text and determine if it was written by a human or generated by artificial intelligence.

Return ONLY a valid JSON in the following format:
{
  "aiScore": number (0-100, where 100 = definitely AI),
  "confidence": "HIGH" | "MEDIUM" | "LOW",
  "isAiGenerated": boolean,
  "indicators": [
    {
      "type": "string",
      "description": "string",
      "severity": "low" | "medium" | "high"
    }
  ],
  "explanation": "string (detailed explanation in English)",
  "suspiciousParts": [
    {
      "text": "string (suspicious excerpt)",
      "score": number (0-100),
      "reason": "string (reason for suspicion)"
    }
  ]
}

Important indicators:
- Repetitive phrases or structures
- Excessive use of connectors ("furthermore", "therefore", "in conclusion")
- Overly formal or artificial language
- Lack of personal opinion or experience
- Very organized and predictable structure
- Excessive use of numbered lists`,
    
    user: (text: string) => `Analyze this text and determine if it was generated by AI:\n\n"${text}"`
  }
}

// Response validation schema
const analysisResponseSchema = {
  aiScore: (value: any) => typeof value === 'number' && value >= 0 && value <= 100,
  confidence: (value: any) => ['HIGH', 'MEDIUM', 'LOW'].includes(value),
  isAiGenerated: (value: any) => typeof value === 'boolean',
  indicators: (value: any) => Array.isArray(value),
  explanation: (value: any) => typeof value === 'string',
  suspiciousParts: (value: any) => Array.isArray(value)
}

// Validate OpenAI response
function validateAnalysisResponse(response: any): boolean {
  if (!response || typeof response !== 'object') return false
  
  return Object.entries(analysisResponseSchema).every(([key, validator]) => {
    return validator(response[key])
  })
}

// Retry mechanism with exponential backoff
async function retryWithBackoff<T>(
  operation: () => Promise<T>,
  maxAttempts: number = OPENAI_SETTINGS.retries.maxAttempts,
  backoffMs: number = OPENAI_SETTINGS.retries.backoffMs
): Promise<T> {
  let lastError: Error = new Error('No attempts made')
  
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error as Error
      
      // Don't retry on certain errors
      if (error instanceof AppError) {
        throw error
      }
      
      if (attempt === maxAttempts) break
      
      // Exponential backoff
      const delay = backoffMs * Math.pow(2, attempt - 1)
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }
  
  throw new AppError(
    `OpenAI API failed after ${maxAttempts} attempts: ${lastError.message}`,
    503,
    ERROR_CODES.INTERNAL_ERROR
  )
}

// Main analysis function
export async function analyzeTextWithOpenAI(
  text: string,
  language: 'pt' | 'en' = 'pt'
): Promise<{
  aiScore: number
  confidence: 'HIGH' | 'MEDIUM' | 'LOW'
  isAiGenerated: boolean
  indicators: AnalysisIndicator[]
  explanation: string
  suspiciousParts: SuspiciousPart[]
  processingTime: number
  wordCount: number
  charCount: number
  tokensUsed: number
  estimatedCost: number
}> {
  const startTime = Date.now()
  
  // Input validation
  if (!text || text.trim().length < 50) {
    throw new AppError(
      'Text is too short for analysis. Minimum 50 characters required.',
      400,
      ERROR_CODES.VALIDATION_ERROR
    )
  }
  
  if (text.length > OPENAI_SETTINGS.limits.maxInputLength) {
    throw new AppError(
      `Text is too long. Maximum ${OPENAI_SETTINGS.limits.maxInputLength} characters allowed.`,
      400,
      ERROR_CODES.VALIDATION_ERROR
    )
  }
  
  const prompt = ANALYSIS_PROMPTS[language]
  const inputTokens = estimateTokens(prompt.system + prompt.user(text))
  const estimatedOutputTokens = 500 // Estimated response size
  
  // Check token limits
  if (inputTokens > OPENAI_SETTINGS.limits.maxTokens * 0.8) {
    throw new AppError(
      'Text is too long for analysis. Please reduce the text size.',
      400,
      ERROR_CODES.VALIDATION_ERROR
    )
  }
  
  try {
    const response = await retryWithBackoff(async () => {
      return await openaiClient.chat.completions.create({
        model: OPENAI_SETTINGS.models.primary,
        messages: [
          { role: 'system', content: prompt.system },
          { role: 'user', content: prompt.user(text) }
        ],
        max_tokens: OPENAI_SETTINGS.limits.maxTokens - inputTokens,
        temperature: 0.3, // Low temperature for consistent analysis
        response_format: { type: 'json_object' },
      })
    })
    
    const content = response.choices[0]?.message?.content
    if (!content) {
      throw new AppError(
        'No response from OpenAI API',
        503,
        ERROR_CODES.INTERNAL_ERROR
      )
    }
    
    let analysisResult
    try {
      analysisResult = JSON.parse(content)
    } catch (error) {
      throw new AppError(
        'Invalid response format from OpenAI API',
        503,
        ERROR_CODES.INTERNAL_ERROR
      )
    }
    
    // Validate response structure
    if (!validateAnalysisResponse(analysisResult)) {
      throw new AppError(
        'Invalid analysis response structure',
        503,
        ERROR_CODES.INTERNAL_ERROR
      )
    }
    
    const processingTime = Date.now() - startTime
    const tokensUsed = response.usage?.total_tokens || (inputTokens + estimatedOutputTokens)
    const estimatedCost = estimateCost(
      response.usage?.prompt_tokens || inputTokens,
      response.usage?.completion_tokens || estimatedOutputTokens,
      OPENAI_SETTINGS.models.primary
    )
    
    const wordCount = text.split(/\s+/).filter(word => word.length > 0).length
    const charCount = text.length
    
    return {
      aiScore: Math.round(analysisResult.aiScore * 100) / 100,
      confidence: analysisResult.confidence,
      isAiGenerated: analysisResult.isAiGenerated,
      indicators: analysisResult.indicators || [],
      explanation: analysisResult.explanation,
      suspiciousParts: analysisResult.suspiciousParts || [],
      processingTime,
      wordCount,
      charCount,
      tokensUsed,
      estimatedCost,
    }
    
  } catch (error) {
    console.error('OpenAI analysis error:', error)
    
    if (error instanceof AppError) {
      throw error
    }
    
    // Handle specific OpenAI errors
    if (error instanceof Error) {
      if (error.message.includes('rate_limit_exceeded')) {
        throw new AppError(
          'Rate limit exceeded. Please try again later.',
          429,
          ERROR_CODES.RATE_LIMIT_EXCEEDED
        )
      }
      
      if (error.message.includes('insufficient_quota')) {
        throw new AppError(
          'OpenAI quota exceeded. Please contact support.',
          503,
          ERROR_CODES.INTERNAL_ERROR
        )
      }
    }
    
    throw new AppError(
      'Analysis service temporarily unavailable. Please try again later.',
      503,
      ERROR_CODES.INTERNAL_ERROR
    )
  }
}

// Fallback analysis for when OpenAI is unavailable
export async function fallbackAnalysis(
  text: string,
  language: 'pt' | 'en' = 'pt'
): Promise<{
  aiScore: number
  confidence: 'LOW'
  isAiGenerated: boolean
  indicators: AnalysisIndicator[]
  explanation: string
  suspiciousParts: SuspiciousPart[]
  processingTime: number
  wordCount: number
  charCount: number
  tokensUsed: number
  estimatedCost: number
}> {
  const startTime = Date.now()
  
  // Basic heuristic analysis (simplified version of original fake logic)
  const words = text.toLowerCase().split(/\s+/)
  const uniqueWords = new Set(words)
  const vocabularyRatio = uniqueWords.size / words.length
  
  let aiScore = 30 // Base score
  const indicators: AnalysisIndicator[] = []
  
  if (vocabularyRatio < 0.5) {
    aiScore += 20
    indicators.push({
      type: 'repetitive_vocabulary',
      description: language === 'pt' 
        ? 'Alta repetição no vocabulário sugere geração por IA'
        : 'High vocabulary repetition suggests AI generation',
      severity: 'medium',
    })
  }
  
  // Add some randomness but keep it predictable
  aiScore += Math.sin(text.length) * 10
  aiScore = Math.max(0, Math.min(100, aiScore))
  
  return {
    aiScore: Math.round(aiScore * 100) / 100,
    confidence: 'LOW',
    isAiGenerated: aiScore > 60,
    indicators,
    explanation: language === 'pt'
      ? `Análise básica completada. Pontuação de IA: ${Math.round(aiScore)}%. Serviço de análise avançada temporariamente indisponível.`
      : `Basic analysis completed. AI score: ${Math.round(aiScore)}%. Advanced analysis service temporarily unavailable.`,
    suspiciousParts: [],
    processingTime: Date.now() - startTime,
    wordCount: words.length,
    charCount: text.length,
    tokensUsed: 0,
    estimatedCost: 0,
  }
}